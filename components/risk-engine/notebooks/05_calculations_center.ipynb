{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, ArrayType, StringType\n",
    "from pyspark.sql.functions import udf, broadcast\n",
    "\n",
    "\n",
    "from optbinning import OptimalBinning, ContinuousOptimalBinning\n",
    "import category_encoders as ce\n",
    "from typing import List, Dict\n",
    "import shap\n",
    "import lightgbm as lgb\n",
    "import scipy\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, pointbiserialr, f_oneway\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import logistic, norm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths and custom modules\n",
    "cwd_path = Path(os.getcwd())\n",
    "data_path = cwd_path/'data/'\n",
    "objects_path = cwd_path/'objects/'\n",
    "sys.path.append(os.path.dirname(cwd_path))\n",
    "\n",
    "from utils import *\n",
    "\n",
    "data_catalog, models_catalog = return_catalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://amr-tawfik:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x160591cfd10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the pyspark path to env excutable\n",
    "conda_env_path = sys.prefix\n",
    "os.environ['PYSPARK_PYTHON'] = os.path.join(conda_env_path, 'python')\n",
    "\n",
    "# build spark session\n",
    "spark = build_spark_session()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- CC Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PD_THRESHOLD = 0.325\n",
    "GREEN_INCOME_ZONE_THRESHOLD = 0.1\n",
    "RED_INCOME_ZONE_THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "PD_BINS_TO_CWF = {\n",
    "    'pd_in': [PD_THRESHOLD, 0.23, 0.1, 0.04, 0.01, 0],\n",
    "    'cwf_map': [1.5, 2.3, 4, 5, 7],  \n",
    "    'risk_seg_map': ['Tier-5', 'Tier-4', 'Tier-3', 'Tier-2', 'Tier-1']\n",
    "}\n",
    "\n",
    "CL_MAX_LIMIT = 500000\n",
    "CL_MIN_LIMIT = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Fetch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "data_date = '20240810'\n",
    "income_model_version = 'v0.7'\n",
    "pd_model_version = 'v0.7'\n",
    "rule_based_version = 'v0.1'\n",
    "\n",
    "# processed all features\n",
    "df_to_all_models = pd.read_parquet(data_path / f\"features_store/{data_date}_L1_processed_features.parquet\")\n",
    "\n",
    "# income predictions\n",
    "income_predics = pd.read_parquet(data_path / f'models_preds/{data_date}_income_{income_model_version}_predics.parquet')\n",
    "income_predics = income_predics.rename(columns={'predics': 'income_predics'})\n",
    "\n",
    "# pd predictions\n",
    "pd_predics = pd.read_parquet(data_path / f'models_preds/{data_date}_pd_{pd_model_version}_predics.parquet')\n",
    "pd_predics = pd_predics.rename(columns={'cali_predics': 'pd_predics'})\n",
    "\n",
    "# rule-based predictions\n",
    "rule_based_predics = pd.read_parquet(data_path / f'models_preds/{data_date}_rule_based_{rule_based_version}_predics.parquet')\n",
    "rule_based_predics = rule_based_predics.rename(columns={'predics_score': 'rule_based_predics_score',\n",
    "                                                        'predics_pd':'rule_based_predics_pd'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Select Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = df_to_all_models[[\n",
    "    'client_id',\n",
    "    'net_income_inflated',\n",
    "    'fo_par90_flag',\n",
    "    'OpenAccounts_InstallmentAmount_sum',\n",
    "    'net_burden_inflated'\n",
    "    ]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = (\n",
    "    base_df\n",
    "    .merge(income_predics, on='client_id', how='left')\n",
    "    .merge(pd_predics, on='client_id', how='left')\n",
    "    .merge(rule_based_predics, on='client_id', how='left')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Simulation Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PD Accept/Reject Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['pd_AR'] = np.where(\n",
    "    full_df['pd_predics'] <= PD_THRESHOLD,\n",
    "    'accept',\n",
    "    'reject'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get income ratio\n",
    "full_df['income_ratio'] = full_df['net_income_inflated'] / full_df['income_predics']\n",
    "\n",
    "# Get final income and zone\n",
    "full_df['final_income'], full_df['income_zone'] = calculate_income_zones(full_df,\n",
    "                                                      'net_income_inflated',\n",
    "                                                      'income_predics',\n",
    "                                                      blogic='all_not_verify',\n",
    "                                                      GREEN_INCOME_ZONE_THRESHOLD=GREEN_INCOME_ZONE_THRESHOLD,\n",
    "                                                      RED_INCOME_ZONE_THRESHOLD=RED_INCOME_ZONE_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Creditworthiness scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cwf_and_segment(pd_series, ar_series, pd_bins_to_cwf=PD_BINS_TO_CWF):\n",
    "    \"\"\"\n",
    "    Determine the Tier and Creditworthiness Factor (CWF) for accepted customers.\n",
    "    If the customer is rejected (based on ar_series), return 0 and 'Rejected'.\n",
    "    Edge cases where PD is higher than max or lower than min are mapped to Tier-5 and Tier-1, respectively.\n",
    "\n",
    "    Parameters:\n",
    "    pd_series (pd.Series): Series containing the probability of default (PD) values.\n",
    "    ar_series (pd.Series): Series indicating 'accept' or 'reject' for each customer.\n",
    "    pd_bins_to_cwf (dict): Dictionary containing the PD bins, CWF mappings, and risk segments.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series, pd.Series: Two Series - cwf and risk_segment.\n",
    "    \"\"\"\n",
    "    # Initialize default values for rejection\n",
    "    results = [{'cwf': 0, 'risk_segment': 'Rejected'} if ar.lower() == 'reject' else None \n",
    "               for ar in ar_series]\n",
    "\n",
    "    # Map accepted customers\n",
    "    for i, (pd_value, ar_flag) in enumerate(zip(pd_series, ar_series)):\n",
    "        if results[i] is not None:  # Skip rejected cases\n",
    "            continue\n",
    "\n",
    "        # Edge case: PD is higher than the maximum bin\n",
    "        if pd_value > pd_bins_to_cwf['pd_in'][0]:\n",
    "            results[i] = {\n",
    "                'cwf': pd_bins_to_cwf['cwf_map'][0],\n",
    "                'risk_segment': pd_bins_to_cwf['risk_seg_map'][0]\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        # Edge case: PD is smaller than the minimum bin\n",
    "        if pd_value < pd_bins_to_cwf['pd_in'][-1]:\n",
    "            results[i] = {\n",
    "                'cwf': pd_bins_to_cwf['cwf_map'][-1],\n",
    "                'risk_segment': pd_bins_to_cwf['risk_seg_map'][-1]\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        # Accepted cases: Map to corresponding bin\n",
    "        for j in range(len(pd_bins_to_cwf['pd_in']) - 1):\n",
    "            if pd_bins_to_cwf['pd_in'][j + 1] < pd_value <= pd_bins_to_cwf['pd_in'][j]:\n",
    "                results[i] = {\n",
    "                    'cwf': pd_bins_to_cwf['cwf_map'][j],\n",
    "                    'risk_segment': pd_bins_to_cwf['risk_seg_map'][j]\n",
    "                }\n",
    "                break\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Return cwf and risk_segment as separate Series\n",
    "    return results_df['cwf'], results_df['risk_segment']\n",
    "\n",
    "\n",
    "full_df['cwf'], full_df['cwf_segment'] = get_cwf_and_segment(full_df['pd_predics'], full_df['pd_AR'],PD_BINS_TO_CWF)\n",
    "\n",
    "# add reject flag\n",
    "full_df.loc[full_df['pd_AR'] == 'reject', 'cwf_segment'] = 'Rejected'\n",
    "\n",
    "#\n",
    "cat_order = ['Rejected']+PD_BINS_TO_CWF['risk_seg_map']\n",
    "full_df['cwf_segment'] = full_df['cwf_segment'].astype('category')\n",
    "full_df['cwf_segment'] = full_df['cwf_segment'].cat.reorder_categories(cat_order, ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Credit Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['final_net_income'] = full_df['final_income'] - (full_df['OpenAccounts_InstallmentAmount_sum'].fillna(0) + full_df['net_burden_inflated'].fillna(0))\n",
    "full_df['final_net_income'] = full_df['final_net_income'].clip(lower=0)\n",
    "full_df['credit_limit'] = round(full_df['final_net_income'] * full_df['cwf'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['client_id', 'net_income_inflated', 'fo_par90_flag',\n",
       "       'OpenAccounts_InstallmentAmount_sum', 'net_burden_inflated',\n",
       "       'income_predics', 'pd_predics', 'rule_based_predics_score',\n",
       "       'rule_based_predics_pd', 'pd_AR', 'income_ratio', 'final_income',\n",
       "       'income_zone', 'cwf', 'cwf_segment', 'final_net_income',\n",
       "       'credit_limit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7- Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['client_id', 'pd_AR', 'income_ratio',\n",
    "                     'final_income', 'income_zone', 'cwf', 'cwf_segment',\n",
    "                     'final_net_income', 'credit_limit']\n",
    "\n",
    "df_out = full_df[selected_features].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v0.2'\n",
    "\n",
    "df_out.to_parquet(data_path / f'models_preds/{data_date}_calculation_center_{version}_results.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
